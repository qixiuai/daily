from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensor2tensor.layers import common_hparams
from tensor2tensor.layers import common_layers
from tensor2tensor.utils import registry
from tensor2tensor.utils import t2t_model
from tensor2tensor.models.shake_shake import shakeshake_small

import tensorflow as tf

@registry.register_model
class SleepStage(t2t_model.T2TModel):

    def body(self, features):
        hparams = self.hparams
        x = features["inputs"]
        shape = common_layers.shape_list(x)
        x = tf.reshape(x, [-1, shape[1] * shape[2] * shape[3]])
        for i in range(hparams.num_hidden_layers):
            x = tf.layers.dense(x, hparams.hidden_size, name="layer_%d" % i)
            x = tf.nn.dropout(x, keep_prob=1.0 - hparams.dropout)
            x = tf.nn.relu(x)
        return tf.expand_dims(tf.expand_dims(x, axis=1), axis=1)

    
@registry.register_model
class VGGNet(t2t_model.T2TModel):

    def body(self, features):
        hparams = self.hparams
        x = features["inputs"]

@registry.register_hparams
def sleepstage_basic():
    hparams = common_hparams.basic_params1()
    hparams.initializer = 'uniform_unit_scaling'
    hparams.num_hidden_layers = 5
    hparams.hidden_size = 500
    hparams.dropout = 0.6
    hparams.learning_rate = 0.001
    hparams.batch_size = 512
    return hparams

@registry.register_hparams
def shakeshake_basic():
    hparams = shakeshake_small()
    hparams.optimizer = "Adam"
    hparams.learning_rate = 0.001
    hparams.learning_rate_cosine_cycle_steps = 10000
    hparams.shake_shake_num_branches = 6
    hparams.hidden_size = 32
    hparams.num_hidden_layers = 14
    hparams.batch_size = 32
    return hparams
