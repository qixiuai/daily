
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import contextlib
import copy
import os
import sys

import numpy as np

from absl import app
from absl import flags
from absl import logging

from tensor2tensor.layers import common_layers

from deepstage import problems
from deepstage import models
from deepstage.utils import registry
from deepstage.utils import trainer_lib

import tensorflow as tf
tf.enable_eager_execution()
tfe = tf.contrib.eager
Modes = tf.estimator.ModeKeys

import pdb

FLAGS = flags.FLAGS

import random

tf.set_random_seed(2018)
np.random.seed(2018)
random.seed(2018)

flags.DEFINE_string('data_dir', '/home/guo/data/tmp/deepstage/data_dir', 'data_dir')
flags.DEFINE_string("tmp_dir", "/home/guo/data/tmp/deepstage/tmp_dir", "tmp_dir")
flags.DEFINE_string("train_dir", "/home/guo/data/tmp/deepstage/train_dir", "train_dir")


def generate_data():
    tf.gfile.MakeDirs(FLAGS.data_dir)
    tf.gfile.MakeDirs(FLAGS.tmp_dir)
    tf.gfile.MakeDirs(FLAGS.train_dir)
    isruc = problems.problem("stage_isruc")    
    isruc.generate_data(FLAGS.data_dir, FLAGS.tmp_dir)

def basic_fc_model(inputs):
#    is_training = hparams.mode == tf.contrib.learn.ModeKeys.TRAIN
    shape = common_layers.shape_list(inputs)
    x = tf.reshape(inputs, [-1, shape[1] * shape[2] * shape[3]])
    n = num_hidden_layers = 3
    for i in range(n):
        hidden_size = 200
        x = tf.layers.dense(x, hidden_size, name="layer_%d" % i)
        x = tf.nn.relu(x)
    return tf.expand_dims(tf.expand_dims(x, axis=1), axis=1)    


def top(body_output):
    with tf.variable_scope("top"):
        x = body_output
        x = tf.reduce_mean(x, axis=[1, 2], keepdims=True)
        res = tf.layers.dense(x, 10)
        return res


def loss_fn(top_out, targets):
    loss_scale = tf.losses.softmax_cross_entropy(
        onehot_labels=targets, logits=top_out)
    return loss_scale

@tfe.implicit_value_and_gradients
def model(examples):
    inputs = examples["inputs"]
    targets = examples["targets"]
    x = inputs
#    x = basic_fc_model(x)
    logits = top(x)
    loss = loss_fn(logits, targets)
    return loss

def eager_train():
    stage_isruc = problems.problem("stage_isruc")
    dataset = tfe.Iterator(stage_isruc.dataset(Modes.TRAIN, FLAGS.data_dir).repeat(1000).batch(512))
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
    for i, examples in enumerate(dataset):
#        with tf.GradientTape() as tap:
        loss, grads_and_vars = model(examples)
        #    tf.contrib.summary.scalar("loss", loss)
        optimizer.apply_gradients(grads_and_vars)
        print(loss)
        if i == 5:
            break
            pdb.set_trace()
        i += 1

def mnist_train():
    l = [0 for i in range(10)]
    image_mnist = problems.problem("image_mnist")
    image_mnist.generate_data(FLAGS.data_dir, FLAGS.tmp_dir)
    dataset = tfe.Iterator(image_mnist.dataset(Modes.TRAIN, FLAGS.data_dir).repeat(1000).batch(512))
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
    for i, examples in enumerate(dataset):
#        with tf.GradientTape() as tap:
        print(examples["targets"])
        targets = examples["targets"]

        loss, grads_and_vars = model(examples)
        #    tf.contrib.summary.scalar("loss", loss)
        optimizer.apply_gradients(grads_and_vars)
        print(loss)
        if i == 5:
            break
            pdb.set_trace()
        i += 1
        

def train():
    stage_isruc = problems.problem("stage_isruc")
    dataset = stage_isruc.dataset(Modes.TRAIN, FLAGS.data_dir).repeat(1000).batch(512)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
    #iterator = dataset.make_initializable_iterator()
    iterator = dataset.make_one_shot_iterator()
    examples = iterator.get_next()
    loss = model(examples)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        for i in range(20):
            val = sess.run([train_op, loss])
            logging.info(val)
        
def main(unused):
    del unused
    #generate_data()
    #train()
    mnist_train()
    
if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.ERROR)
    app.run(main)
