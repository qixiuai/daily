
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D
from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import Concatenate
from tensorflow.python.ops.init_ops_v2 import he_normal, he_uniform
from tensorflow import initializers

"""
class BatchNormalization(tf.keras.layers.Layer):

    def __init__(self,
                 axis=-1,
                 momentum=0.99,
                 epsilon=1e-3,
                 center=True,
                 scale=True,
                 beta_initializer='zeros',
                 gamma_initializer='ones',
                 moving_mean_initializer='zeros',
                 moving_variance_initializer='ones',
                 beta_regularizer=None,
                 gamma_regularizer=None,
                 beta_constraint=None,
                 gamma_constraint=None,
                 renorm=False,
                 renorm_clipping=None,
                 renorm_momentum=0.99,
                 trainable=True,
                 virtual_batch_size=None,
                 adjustment=None,
                 name=None,
                 **kwargs):
        super(BatchNormalization, self).__init__(
            name=name, **kwargs)
        if isinstance(axis, list):
            self.axis = axis[:]
        elif isinstance(axis, int):
            self.axis = axis
        else:
            raise TypeError('axis must  be int or list,  type given: {}'.format(type(axis)))

        self.momentum = momentum
        self.epsilon = epsilon
        self.center = center
        self.scale = scale
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)

    def build(self, input_shape):
        self.built = True
        input_shape = tensor_shape.TensorShape(input_shape)
        self.gamma = self.add_weight(
            name='gamma',
            shape=param_shape,
            dtype=self._param_dtype,
            initializer=self.gamma_initializer,
            regularizer=self.gamma_regularizer,
            constraint=self.gamma_constraint,
            trainable=True,
            experimental_autocast=False)
        self.beta = self.add_weight(
            name='beta',
            shape=param_shape,
            dtype=self._param_dtype,
            initializer=self.beta_initializer,
            regularizer=self.beta_regularizer,
            constraint=self.beta_constraint,
            trainable=True,
            experimental_autocast=False)
        self.moving_mean = self.add_weight(
            name='moving_mean',
            shape=param_shape,
            dtype=self._param_dtype,
            initializer=self.beta_initializer,
            regularizer=self.beta_regularizer,
            constraint=self.beta_constraint,
            trainable=False,
            experimental_autocast=False)
        
        self.rolling_var = None
        

    def call(self, inputs, training=None):
        if mode == 'train':
            sample_mean = tf.mean(inputs, axis=0, keepdims=True)
            sample_var = tf.var(inputs, axis=0, keepdims=True) + eps
            x_norm = (x - sample_mean) / tf.sqrt(sample_var)
            out = gamma.reshape(1,-1) * x_norm + beta.reshape(1, -1)
            running_mean = momentum * running_mean + (1 - momentum) * sample_mean
            running_var = momentum * running_var + (1 - momentum) * sample_var
        elif mode == 'test':
            out = (x - running_mean.reshape(1, -1)) / tf.sqrt(running_var.reshape(1,-1) + eps)
        else:
            raise ValueError('Invalid forward batchnorm mode {}'.format(mode))
        return out
"""

class Conv1DBN(tf.keras.Model):

    def __init__(self,
                 filters=None,
                 kernel_size=None,
                 strides=1,
                 activation="relu"):
        super(Conv1DBN, self).__init__()
        self.conv1d = Conv1D(filters=filters,
                             kernel_size=kernel_size,
                             strides=strides,
                             padding="same", 
                             kernel_initializer=he_uniform(),
                             activation="relu")
        self.bn = BatchNormalization()

    def call(self, x):
        x = self.conv1d(x)
        x = self.bn(x)
        return x


class LowFreqBlock(tf.keras.Model):

    def __init__(self):
        super(LowFreqBlock, self).__init__()
        self.conv1d_bn_1 = Conv1DBN(filters=64, kernel_size=400, strides=50)
        self.maxpooling1d_1 = MaxPooling1D(pool_size=4, strides=4, padding="same")
        self.dropout = Dropout(0.5)
        self.conv1d_bn_layers = [Conv1DBN(filters=128, kernel_size=6, strides=1) for _ in range(3)]
        self.maxpooling1d_2 = MaxPooling1D(pool_size=2, strides=2, padding="same")
        self.flatten = Flatten()

    def call(self, x):
        x = self.conv1d_bn_1(x)
        x = self.maxpooling1d_1(x)
        x = self.dropout(x)
        for conv1d_bn in self.conv1d_bn_layers:
            x = conv1d_bn(x)
        x = self.maxpooling1d_2(x)
        x = self.flatten(x)
        return x


class HighFreqBlock(tf.keras.Model):

    def __init__(self):
        super(HighFreqBlock, self).__init__()
        self.conv1d_bn_1 = Conv1DBN(filters=64, kernel_size=50, strides=6)
        self.maxpooling1d_1 = MaxPooling1D(pool_size=8, strides=8, padding="same")
        self.dropout = Dropout(0.5)
        self.conv1d_bn_layers = [Conv1DBN(filters=128, kernel_size=8, strides=1) for _ in range(3)]
        self.maxpooling1d_2 = MaxPooling1D(pool_size=4, strides=4, padding="same")
        self.flatten = Flatten()

    def call(self, x):
        x = self.conv1d_bn_1(x)
        x = self.maxpooling1d_1(x)
        x = self.dropout(x)
        for conv1d_bn in self.conv1d_bn_layers:
            x = conv1d_bn(x)
        x = self.maxpooling1d_2(x)
        x = self.flatten(x)
        return x



class DeepStageNet(tf.keras.Model):

    def __init__(self):
        super(DeepStageNet, self).__init__()
        self.low_freq_block = LowFreqBlock()
        self.high_freq_block = HighFreqBlock()
        self.concatenate = Concatenate(axis=1)
        self.dropout = Dropout(0.5)
        self.softmax = tf.keras.layers.Dense(5, activation="softmax")

    @tf.function
    def call(self, x):
        low_freq = self.low_freq_block(x)
        high_freq = self.high_freq_block(x)
        freqs = self.concatenate([low_freq, high_freq])
        x = self.dropout(freqs)
        x = self.softmax(x)
        return x


if __name__ == '__main__':
    import pdb
    net = DeepStageNet()
    variables = net.trainable_variables
    print(variables)
    pdb.set_trace()
