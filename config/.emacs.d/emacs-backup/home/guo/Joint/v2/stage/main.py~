
import pdb
import shutil
import datetime
from isruc import RawIsruc
from model import DeepStageNet
from metrics import eval_confusion_matrix
from tensorflow.keras.optimizers.schedules import ExponentialDecay
import tensorflow as tf

from absl import flags
from absl import app


@tf.function
def train_step(input, target, model, loss_fn, optimizer, train_loss, train_acc, train_conf=None):
    with tf.GradientTape() as tape:
        pred = model(input)
        loss = loss_fn(target, pred)
    variables = model.trainable_variables
    grads = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(grads, variables))
    train_loss(loss)
    train_acc(target, pred)
    if train_conf:
        train_conf(target, pred)

@tf.function
def valid_step(input, target, model, loss_fn, valid_loss, valid_acc, valid_conf=None):
    pred = model(input)
    loss = loss_fn(target, pred)
    valid_loss(loss)
    valid_acc(target, pred)
    if valid_conf:
        valid_conf(target, pred)


def main(unused_args):
    del unused_args

    debug = False
    init_from_scratch = False

    if debug:
        NUM_EPOCHS = 10
        BATCH_SIZE = 64
        NUM_BATCHES_PER_EPOCH = 5
    else:
        NUM_EPOCHS = 1001
        BATCH_SIZE = 512
        NUM_BATCHES_PER_EPOCH = 10000000
    
    BUFFER_SIZE = 10 * BATCH_SIZE

    train_isruc = RawIsruc(mode="train")
    valid_isruc = RawIsruc(mode="validation")
    train_dataset = tf.data.Dataset.from_generator(train_isruc, (tf.float32, tf.int64))
    valid_dataset = tf.data.Dataset.from_generator(valid_isruc, (tf.float32, tf.int64))
    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)
    valid_dataset = valid_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)

    model = DeepStageNet()
    for example, target in train_dataset.take(1):
        model(example)
    #pdb.set_trace()
    optimizer = tf.keras.optimizers.Adam(learning_rate=ExponentialDecay(initial_learning_rate=1e-4,
                                                                        decay_steps=1000,
                                                                        decay_rate=0.99))
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
    if debug:
        data_dir = "/home/guo/Joint/tmp/"
    else:
        data_dir = "/home/guo/Joint/v2/stage/"
    summary_id = datetime.datetime.now().ctime()
    train_summary_writer = tf.summary.create_file_writer(data_dir + "summaries/train" + summary_id)
    valid_summary_writer = tf.summary.create_file_writer(data_dir + "summaries/valid" + summary_id)

    train_loss = tf.keras.metrics.Mean("train_loss", dtype=tf.float32)
    valid_loss = tf.keras.metrics.Mean("valid_loss", dtype=tf.float32)
    train_acc_mean = tf.keras.metrics.Mean("train_acc_mean", dtype=tf.float32)
    valid_acc_mean = tf.keras.metrics.Mean("valid_acc_mean", dtype=tf.float32)
    
    train_acc = tf.keras.metrics.SparseCategoricalAccuracy("train_acc", dtype=tf.float32)
    valid_acc = tf.keras.metrics.SparseCategoricalAccuracy("valid_acc", dtype=tf.float32)

    #train_conf = ConfusionMatrixAccuracy("train_confusion_matrix", dtype=tf.float32)
    #valid_conf = ConfusionMatrixAccuracy("valid_confusion_matrix", dtype=tf.float32)
    
    def write_summaries(writer, loss, acc, acc_mean, step):
        with writer.as_default():
            tf.summary.scalar("loss", loss.result(), step=step)
            tf.summary.scalar("acc", acc.result(), step=step)
            tf.summary.scalar("acc_mean", acc_mean.result(), step=step)
            
    for example, target in train_dataset.take(1):
        train_step(example, target, model, loss_fn, optimizer, train_loss, train_acc)

    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)
    ckpt_dir = data_dir + "ckpts"
    if init_from_scratch:
        shutil.rmtree(ckpt_dir, ignore_errors=True)
    manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=20)
    ckpt.restore(manager.latest_checkpoint)
    if manager.latest_checkpoint:
        print("Restored from {}".format(manager.latest_checkpoint))
    else:
        print("Initializing from scratch.")

    is_confusion_matrix = False
    if is_confusion_matrix:
        eval_confusion_matrix(valid_dataset, model)

    epoch_id = ckpt.step.numpy()
    while epoch_id < NUM_EPOCHS:
        for id, (example, target) in enumerate(train_dataset.take(NUM_BATCHES_PER_EPOCH)):
            train_step(example, target, model, loss_fn, optimizer, train_loss, train_acc)
            train_acc_mean(train_acc.result())

        for example, target in valid_dataset.take(NUM_BATCHES_PER_EPOCH):
            valid_step(example, target, model, loss_fn, valid_loss, valid_acc)
            valid_acc_mean(valid_acc.result())

        step = epoch_id
        write_summaries(train_summary_writer, train_loss, train_acc, train_acc_mean, step)
        write_summaries(valid_summary_writer, valid_loss, valid_acc, valid_acc_mean, step)

        ckpt.step.assign_add(1)
        if epoch_id >= 0:
            manager.save()

        print("epoch: {:3d}, train_loss: {:.4f}, train_acc: {:.4f}, train_acc_mean: {:.4f}, \
        valid_loss: {:.4f}, valid_acc: {:.4f}, valid_acc_mean: {:.4f} ".format(
                   epoch_id, train_loss.result(), train_acc.result(), train_acc_mean.result(),
                   valid_loss.result(), valid_acc.result(), valid_acc_mean.result()))

        train_loss.reset_states()
        valid_acc.reset_states()
        train_acc_mean.reset_states()
        valid_acc_mean.reset_states()
        epoch_id = ckpt.step.numpy()
        
    call = model.__call__.get_concrete_function(tf.TensorSpec((None, 6000, 5), tf.float32))
    tf.saved_model.save(model, data_dir+"saved_models", signatures=call)
    #tf.keras.experimental.export_saved_model(model, 'saved_models', serving_only=True)

if __name__ == '__main__':
    app.run(main)




