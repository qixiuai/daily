
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, TimeDistributed
from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU
from tensorflow.keras.layers import Concatenate
from tensorflow.python.ops.init_ops_v2 import he_normal, he_uniform
from tensorflow import initializers
from tensorflow.keras import backend as K


class Conv1DBN(tf.keras.layers.Layer):

    def __init__(self,
                 filters=None,
                 kernel_size=None,
                 strides=1):
        super(Conv1DBN, self).__init__()
        self.conv1d = Conv1D(filters=filters,
                             kernel_size=kernel_size,
                             strides=strides,
                             padding="same",
                             kernel_initializer=he_uniform())
                             #kernel_regularizer=tf.keras.regularizers.l2(1e-3))
        self.bn = BatchNormalization()
        self.act = ReLU()
        #self.act = LeakyReLU()

    def call(self, x):
        x = self.conv1d(x)
        x = self.bn(x)
        x = self.act(x)
        return x

class LowFreqBlock(tf.keras.layers.Layer):

    def __init__(self):
        super(LowFreqBlock, self).__init__()
        self.conv1d_bn_1 = Conv1DBN(filters=64, kernel_size=800, strides=100)
        self.maxpooling1d_1 = MaxPooling1D(pool_size=4, strides=4, padding="same")
        self.dropout = Dropout(0.5)
        self.conv1d_bn_layers = [Conv1DBN(filters=128, kernel_size=6, strides=1) for _ in range(3)]
        self.maxpooling1d_2 = MaxPooling1D(pool_size=2, strides=2, padding="same")
        self.flatten = Flatten()

    def call(self, x):
        x = self.conv1d_bn_1(x)
        x = self.maxpooling1d_1(x)
        x = self.dropout(x)
        for conv1d_bn in self.conv1d_bn_layers:
            x = conv1d_bn(x)
        x = self.maxpooling1d_2(x)
        x = self.flatten(x)
        return x


class HighFreqBlock(tf.keras.layers.Layer):

    def __init__(self):
        super(HighFreqBlock, self).__init__()
        self.conv1d_bn_1 = Conv1DBN(filters=64, kernel_size=100, strides=12)
        self.maxpooling1d_1 = MaxPooling1D(pool_size=8, strides=8, padding="same")
        self.dropout = Dropout(0.5)
        self.conv1d_bn_layers = [Conv1DBN(filters=128, kernel_size=8, strides=1) for _ in range(3)]
        self.maxpooling1d_2 = MaxPooling1D(pool_size=4, strides=4, padding="same")
        self.flatten = Flatten()

    def call(self, x):
        x = self.conv1d_bn_1(x)
        x = self.maxpooling1d_1(x)
        x = self.dropout(x)
        for conv1d_bn in self.conv1d_bn_layers:
            x = conv1d_bn(x)
        x = self.maxpooling1d_2(x)
        x = self.flatten(x)
        return x


class DeepStageNet(tf.keras.layers.Layer):

    def __init__(self):
        super(DeepStageNet, self).__init__()
        self.low_freq_block = LowFreqBlock()
        self.high_freq_block = HighFreqBlock()
        self.concatenate = Concatenate(axis=1)
        self.dropout = Dropout(0.5)
        #self.softmax = tf.keras.layers.Dense(5, activation="softmax")

    def call(self, x):
        low_freq = self.low_freq_block(x)
        high_freq = self.high_freq_block(x)
        x = self.concatenate([low_freq, high_freq]) 
        x = self.dropout(x)
        #x = self.softmax(x)
        return x


class DeepStageNet1D(tf.Module):

    def __init__(self):
        super(DeepStageNet1D, self).__init__()
        self.cnn = DeepStageNet()
        self.softmax = tf.keras.layers.Dense(5, activation="softmax")

    @tf.function
    def __call__(self, x, training=True):
        K.set_learning_phase(training)
        x = self.cnn(x)
        x = self.softmax(x)
        return x

"""
class CNNLayer(tf.keras.layers.Layer):

    def __init__(self):
        super(CNNLayer, self).__init__()
        self.cnn = DeepStageNet()
        self.built = False

    def call(self, x):
        return self.cnn(x)
"""

class DeepSleepNet(tf.Module):

    def __init__(self):
        super(DeepSleepNet, self).__init__()
        self.cnn = DeepStageNet()
        self.lstm1 = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(512, return_sequences=True, recurrent_initializer='glorot_uniform'))
        self.lstm2 = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(512, return_sequences=True, recurrent_initializer='glorot_uniform'))
        self.fc = Dense(1024, kernel_initializer=he_uniform())
        #self.fc2 = TimeDistributed(tf.keras.layers.Dense(1024, kernel_initializer=he_uniform()))
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.softmax = tf.keras.layers.Dense(5, activation="softmax")
        #self.flatten = TimeDistributed(Flatten())
        self.ln1 = tf.keras.layers.LayerNormalization()
        self.ln2 = tf.keras.layers.LayerNormalization()
        self.bn1 = BatchNormalization()
        self.bn2 = BatchNormalization()
        
    @tf.function
    def __call__(self, x, training=True):
        K.set_learning_phase(training)
        step = x.shape[1]
        x = tf.stack([self.cnn(x[:,ind]) for ind in range(step)], axis=1)
        y = tf.stack([self.fc(x[:,ind]) for ind in range(step)], axis=1)
        x = self.lstm1(x)
        #x = self.dropout(x)
        #x = self.ln1(x)
        x = self.bn1(x)
        x = self.lstm2(x)
        #x = self.ln2(x)
        #x = self.dropout(x)
        x = self.bn2(x)
        x = tf.add(x, y) # BUG here
        x = tf.stack([self.softmax(x[:, ind]) for ind in range(step)], axis=1)
        return x

if __name__ == '__main__':
    import pdb
    net = DeepStageNet()
    variables = net.trainable_variables
    print(variables)
    pdb.set_trace()


