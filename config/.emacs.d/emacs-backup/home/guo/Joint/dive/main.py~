
import numpy as np
import pdb
import time
import shutil
import datetime
from dataset.isruc import RawIsruc
from data_client import DataClient
from model.deepsleepnet import DeepStageNet, DeepSleepNet, DeepStageNet1D
from eval import eval_confusion_matrix
from tensorflow.keras.optimizers.schedules import ExponentialDecay
import tensorflow as tf

from absl import flags
from absl import app

def cross_entropy(target, pred):
    pred_log = tf.math.log(pred + 1e-8)
    n = target.shape[0]
    step = target.shape[1]
    loss = 0
    for ind_sample in range(n):
        for ind_step in range(step):
            target_sample = target[ind_sample][ind_step]
            pred_log_sample = pred_log[ind_sample][ind_step]
            loss = loss - pred_log_sample[target_sample]
    loss = loss / (n * step)
    return loss


@tf.function
def train_step(input, target, sample_weights, model, loss_fn, optimizer, train_loss, train_acc, train_conf=None):
    with tf.GradientTape() as tape:
        pred = model(input, training=True)
        loss = loss_fn(target, pred, sample_weight=sample_weights)
    variables = model.trainable_variables
    grads = tape.gradient(loss, variables)
    #pdb.set_trace()
    grads = list(map(lambda grad:tf.clip_by_value(grad, -5.0, 5.0), grads))
    optimizer.apply_gradients(zip(grads, variables))
    train_loss(loss)
    train_acc(target, pred)
    if train_conf:
        train_conf(target, pred)

@tf.function
def valid_step(input, target, sample_weights, model, loss_fn, valid_loss, valid_acc, valid_conf=None):
    pred = model(input, training=False)
    loss = loss_fn(target, pred, sample_weight=sample_weights)
    valid_loss(loss)
    valid_acc(target, pred)
    if valid_conf:
        valid_conf(target, pred)


def main(unused_args):
    del unused_args

    debug = True
    init_from_scratch = False

    STEP = 25
    if debug:
        NUM_EPOCHS = 10000
        BATCH_SIZE = 128
        NUM_BATCHES_PER_EPOCH = 10000000
    else:
        NUM_EPOCHS = 1001
        BATCH_SIZE = 512
        NUM_BATCHES_PER_EPOCH = 10000000
    channels=["C3_A2", "O1_A2", "LOC_A2", "ROC_A1", "X1"]
    #channels=["C3_A2", "C4_A1", "LOC_A2", "ROC_A1", "X1"]
    #channels=["C3_A2", "LOC_A2", "X1"]
    BUFFER_SIZE = 5 * BATCH_SIZE
    train_isruc = RawIsruc(mode="train", step=STEP, class_balanced=False, channels=channels)
    valid_isruc = RawIsruc(mode="validation", step=STEP, class_balanced=False, channels=channels)
    #train_isruc = DataClient(batch_size=BATCH_SIZE, step=STEP, mode="train", channels=channels)
    #valid_isruc = DataClient(batch_size=BATCH_SIZE, step=STEP, mode="validation", channels=channels)
    train_dataset = tf.data.Dataset.from_generator(train_isruc, (tf.float32, tf.int64, tf.float32))
    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    valid_dataset = tf.data.Dataset.from_generator(valid_isruc, (tf.float32, tf.int64, tf.float32))
    valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)
    valid_dataset = valid_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)

    model = DeepSleepNet()
    #model = DeepStageNet1D()
    for example, _, _ in train_dataset.take(1):
        model(example, training=True)

    optimizer = tf.keras.optimizers.Adam(learning_rate=ExponentialDecay(initial_learning_rate=1e-4,
                                                                        decay_steps=5000,
                                                                        decay_rate=0.99))

    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
    if debug:
        data_dir = "/home/guo/Joint/tmp/"
    else:
        data_dir = "/home/guo/Joint/v2/stage/"
    summary_id = datetime.datetime.now().ctime()
    tag = "lstm_framework + without_mean + full_model"
    train_summary_writer = tf.summary.create_file_writer(data_dir + "summaries/train" + summary_id + tag)
    valid_summary_writer = tf.summary.create_file_writer(data_dir + "summaries/valid" + summary_id + tag)

    train_loss = tf.keras.metrics.Mean("train_loss", dtype=tf.float32)
    valid_loss = tf.keras.metrics.Mean("valid_loss", dtype=tf.float32)
    train_acc_mean = tf.keras.metrics.Mean("train_acc_mean", dtype=tf.float32)
    valid_acc_mean = tf.keras.metrics.Mean("valid_acc_mean", dtype=tf.float32)

    train_acc = tf.keras.metrics.SparseCategoricalAccuracy("train_acc", dtype=tf.float32)
    valid_acc = tf.keras.metrics.SparseCategoricalAccuracy("valid_acc", dtype=tf.float32)

    #train_conf = ConfusionMatrixAccuracy("train_confusion_matrix", dtype=tf.float32)
    #valid_conf = ConfusionMatrixAccuracy("valid_confusion_matrix", dtype=tf.float32)

    def write_summaries(writer, loss, acc, acc_mean, step):
        with writer.as_default():
            tf.summary.scalar("loss", loss.result(), step=step)
            tf.summary.scalar("acc", acc.result(), step=step)
            tf.summary.scalar("acc_mean", acc_mean.result(), step=step)

    for example, target, sample_weights in train_dataset.take(1):
        train_step(example, target, sample_weights, model, loss_fn, optimizer, train_loss, train_acc)

    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)
    ckpt_dir = data_dir + "ckpts"
    if init_from_scratch:
        shutil.rmtree(ckpt_dir, ignore_errors=True)
    manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=20)
    ckpt.restore(manager.latest_checkpoint)
    if manager.latest_checkpoint:
        print("Restored from {}".format(manager.latest_checkpoint))
    else:
        print("Initializing from scratch.")

    is_confusion_matrix = False
    if is_confusion_matrix:
        eval_confusion_matrix(valid_dataset, model)

    epoch_id = ckpt.step.numpy()
    num_batches = 0
    while epoch_id <= NUM_EPOCHS:
        tic = time.time()
        break
        for example, target, sample_weights in train_dataset.take(NUM_BATCHES_PER_EPOCH):
            train_step(example, target, sample_weights, model, loss_fn, optimizer, train_loss, train_acc)
            train_acc_mean(train_acc.result())
            num_batches += 1

        if epoch_id % 1 == 0:
            valid_acc.reset_states()
            valid_acc_mean.reset_states()
            for example, target, sample_weights in valid_dataset.take(NUM_BATCHES_PER_EPOCH):
                valid_step(example, target, sample_weights, model, loss_fn, valid_loss, valid_acc)
                valid_acc_mean(valid_acc.result())
                num_batches += 1

        if epoch_id % 10 == 0:
            eval_confusion_matrix(valid_dataset, model)

        step = epoch_id
        write_summaries(train_summary_writer, train_loss, train_acc, train_acc_mean, step)
        write_summaries(valid_summary_writer, valid_loss, valid_acc, valid_acc_mean, step)

        ckpt.step.assign_add(1)
        if epoch_id >= 50:
            manager.save()

        toc = time.time()
        dur = num_batches * BATCH_SIZE * STEP / (toc - tic)
        print("epoch: {:3d}, secs:{:.2f} epochs/sec: {:.2f} train_loss: {:.4f}, train_acc: {:.4f}, train_acc_mean: {:.4f},\
  valid_loss: {:.4f}, valid_acc: {:.4f}, valid_acc_mean: {:.4f} ".format(
                   epoch_id, toc - tic, dur, train_loss.result(), train_acc.result(), train_acc_mean.result(),
                   valid_loss.result(), valid_acc.result(), valid_acc_mean.result()))
        train_loss.reset_states()
        train_acc_mean.reset_states()
        epoch_id = ckpt.step.numpy()
        num_batches = 0
    call = model.__call__.get_concrete_function(tf.TensorSpec((None, STEP, 6000, 5), tf.float32))
    tf.saved_model.save(model, data_dir+"saved_models", signatures=call)
    #tf.keras.experimental.export_saved_model(model, 'saved_models', serving_only=True)

if __name__ == '__main__':
    app.run(main)




