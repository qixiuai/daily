
import time
import numpy as np
import datetime
import tensorflow as tf

from absl import app

from heart_isruc import HeartIsruc

import pdb


def build_model(rnn_units=2000):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(rnn_units,
                             return_sequences=True,
                             stateful=True,
                             recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(1),
    ])
    return model

@tf.function
def part_mse(y_pred, target):
    prefix_mask_size = 10
    y_pred = y_pred[prefix_mask_size:]
    target = target[prefix_mask_size:]
    loss = tf.keras.backend.mean(tf.keras.backend.square(y_pred - target))
    return loss

@tf.function
def train_step(input, target, model, optimizer, mse):
    loss = 0
    with tf.GradientTape() as tape:
        y_pred = model(input)
        loss = mse(y_pred, target)
    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    #print("batch_train_loss: {}".format(loss))
    return loss


@tf.function
def test_step(input, target, model, mse):
    y_pred = model(input)
    loss = mse(y_pred, target)
    #print("batch_test_loss: {}".format(loss))
    return loss


def main(unused_args):
    del unused_args
    fs = 200

    heart_isruc = HeartIsruc(step_size=256)
    train_isruc = heart_isruc(mode="train")
    valid_isruc = heart_isruc(mode="valid")

    BATCH_SIZE = 256
    BUFFER_SIZE = 30000
    train_dataset = train_isruc.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
    valid_dataset = valid_isruc.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

    # parameter
    rnn_units = 2000

    # tensorboard
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    log_dir = "/home/guo/physio/log_dir/"
    train_log_dir = log_dir + current_time + "/" + str(rnn_units)  + "/heart.train"
    test_log_dir = log_dir + current_time + "/" + str(rnn_units) + "/heart.test"
    train_summary_writer = tf.summary.create_file_writer(train_log_dir)
    test_summary_writer = tf.summary.create_file_writer(test_log_dir)
    
    model = build_model(rnn_units=rnn_units)
    optimizer = tf.keras.optimizers.Adam()
    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)
    ckpt_dir = '/home/guo/physio/tmp_data/ckpts'
    train_from_scratch = False
    if train_from_scratch:
        os.rmdir(ckpt_dir)
    manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=None)
    ckpt.restore(manager.latest_checkpoint)
    if manager.latest_checkpoint:
        print("Restored from {}".format(manager.latest_checkpoint))
    else:
        print("Initializing from scratch.")

    # Train Models
    num_epochs = 1
    batches_per_epoch = 1000
    batches_per_epoch_test = 100
    for epoch_id in range(num_epochs):
        train_loss = 0
        test_loss = 0
        start = time.time()
        real_baches_train = 0
        real_baches_test = 0
        for example, target in train_dataset.take(batches_per_epoch):
            mean = np.median(example, axis=0)
            example = example - mean
            target = target - mean
            train_loss += train_step(example, target, model, optimizer, part_mse)
            real_baches_train += 1
        for example, target in valid_dataset.take(batches_per_epoch_test):
            mean = np.median(example, axis=0)
            example = example - mean
            target = target - mean
            test_loss += test_step(example, target, model, part_mse)
            real_baches_test += 1
        train_loss /= real_baches_train
        test_loss /= real_baches_test
        #train_loss = np.log10(train_loss)
        #test_loss = np.log10(test_loss)
        elapsed = time.time() - start
        saved_path = manager.save()
        print("Saved checkpoint for step {}: {}".format(int(ckpt.step), saved_path), end=" ")
        print("In Epoch {:3d}, cost secs: {:.3f}, train_loss is {:.4f}, test_loss is {:.4f}".format(
             int(ckpt.step), elapsed, train_loss, test_loss))
        ckpt.step.assign_add(1)
        with train_summary_writer.as_default():
            tf.summary.scalar("loss", train_loss, step=epoch_id)
        with test_summary_writer.as_default():
            tf.summary.scalar("loss", test_loss, step=epoch_id)
    pdb.set_trace()
    tf.saved_model.save(model, 'demo_saved_model')

    
if __name__ == '__main__':
    app.run(main)
    
