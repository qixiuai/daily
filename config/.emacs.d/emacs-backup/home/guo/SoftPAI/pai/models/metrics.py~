
import tensorflow as tf


class SigmoidFocalClassificationLoss(tf.keras.losses.Loss):

    def __init__(self, gamma=2.0, alpha=0.25, **kwargs):
        super(SigmoidFocalClassificationLoss, self).__init__(**kwargs)
        self._gamma = gamma
        self._alpha = alpha

    def call(self, preds, targets):
        per_entry_corss_ent = tf.nn.sigmoid_cross_entropy_with_logits(
            labels=targets, logits=preds)
        probs = tf.sigmoid(preds)
        p_t = targets * preds + (1 - targets) * (1 - preds)
        modulating_factor = 1.0
        if self._gamma:
            modulating_factor = tf.pow(1.0-p_t, self._gamma)
        alpha_weight_factor = 1.0
        if self._alpha is not None:
            alpha_weight_factor = (targets * self._alpha +
                                   (1 - targets) * (1 - self._alpha))
        focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *
                                    per_entry_corss_ent)
        return focal_cross_entropy_loss


class SoftmaxFocalClassificationLoss(tf.keras.losses.Loss):

    def __init__(self, gamma=2.0, alpha=0.25):
        super(SoftmaxFocalClassificationLoss, self).__init__()
        self._gamma = gamma
        self.alpha = alpha

    def _compute_loss(self, preds, targets):
        per_entry_corss_ent = tf.nn.softmax_cross_entropy_with_logits(
            labels=targets, logits=preds)
        probs = tf.nn.softmax(preds)
        # TODO error fix this
        p_t = targets * preds + (1 - targets) * (1 - preds)
        modulating_factor = 1.0
        if self._gamma:
            modulating_factor = tf.pow(1.0-p_t, self._gamma)
        alpha_weight_factor = 1.0
        if self._alpha is not None:
            alpha_weight_factor = (targets * self._alpha +
                                   (1 - targets) * (1 - self._alpha))
        focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *
                                    per_entry_corss_ent)
        return focal_cross_entropy_loss    


def cross_entropy_loss_multilabel(logits, labels, smoothing, vocab_size):
    with tf.name_scope("smoothing_cross_entropy"):
        confidence = 1.0 - smoothing
        low_confidence = (1.0 - confidence) / tf.cast(vocab_size - 1, tf.float32)
        soft_targets = tf.one_hot(
            tf.cast(labels, tf.int32),
            depth=vocab_size,
            on_value=confidence,
            off_value=low_confidence,
        )
        xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=soft_targets)

        normalizing_constant = -(
            confidence * tf.math.log(confidence) +
            tf.cast(vocab_size-1, tf.float32) * low_confidence *
            tf.math.log(low_confidence + 1e-20))

        xentropy -= normalizing_constant

    # remove labels is 0 samples
    #weights = tf.cast(tf.not_equal(labels, 0), tf.float32)
    return xentropy

def cross_entropy_loss(logits, labels, smoothing, vocab_size):
    with tf.name_scope("smoothing_cross_entropy"):
        confidence = 1.0 - smoothing
        low_confidence = (1.0 - confidence) / tf.cast(vocab_size - 1, tf.float32)
        soft_targets = tf.one_hot(
            tf.cast(labels, tf.int32),
            depth=vocab_size,
            on_value=confidence,
            off_value=low_confidence,
        )

        xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=soft_targets)

        normalizing_constant = -(
            confidence * tf.math.log(confidence) +
            tf.cast(vocab_size-1, tf.float32) * low_confidence *
            tf.math.log(low_confidence + 1e-20))

        xentropy -= normalizing_constant

    # remove labels is 0 samples
    #weights = tf.cast(tf.not_equal(labels, 0), tf.float32)
    return xentropy

def accuracy(logits, labels):
    labels = tf.keras.backend.flatten(labels)
    outputs = tf.cast(tf.argmax(logits, axis=-1), tf.int32)
    labels = tf.cast(labels, tf.int32)
    return tf.reduce_mean(tf.cast(tf.equal(outputs, labels), tf.float32))

def single_class_accuracy(logits, labels, klass=0):
    labels = tf.keras.backend.flatten(labels)
    outputs = tf.cast(tf.argmax(logits, axis=-1), tf.int32)
    outputs = tf.cast(tf.equal(outputs, klass), tf.int32)
    labels = tf.cast(tf.equal(labels, klass), tf.int32)
    return tf.cast(tf.reduce_sum(tf.multiply(outputs, labels)), tf.float32) / (tf.cast(tf.reduce_sum(labels), tf.float32) - 1e-6)

def accuracy_wake(logits, labels):
    return single_class_accuracy(logits, labels, klass=0)

def accuracy_n1(logits, labels):
    return single_class_accuracy(logits, labels, klass=1)

def accuracy_n2(logits, labels):
    return single_class_accuracy(logits, labels, klass=2)

def accuracy_n3(logits, labels):
    return single_class_accuracy(logits, labels, klass=3)

def accuracy_rem(logits, labels):
    return single_class_accuracy(logits, labels, klass=4)

class ConfusionMatrix(tf.keras.metrics.Metric):

    def __init__(self, name="confusion_matrix_metrics", **kwargs):
        super(ConfusionMatrix, self).__init__(name=name, **kwargs)
        vocab_size = kwargs.get("vocab_size", 5)
        self.mat = self.add_weight(
            name="confusion_matrix",
            initializer="zeros",
            shape=(vocab_size, vocab_size))

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.mat_batch = None
        
        self.mat.assign_add(mat_batch)

    def result(self):
        return self.mat

def f1score(targets, preds):
    preds = tf.cast(tf.greater_equal(preds, 0.5), tf.float32)
    targets = tf.cast(tf.greater_equal(targets, 0.5), tf.float32)
    num_preds = tf.reduce_sum(preds)
    num_truth = tf.reduce_sum(targets)
    preds = preds - 1 # (-1, 0)
    targets = targets * -1 + 1 # (1, 0)
    num_correct_pred = tf.reduce_sum(tf.cast(tf.equal(preds, targets), tf.float32))
    p = num_correct_pred / num_preds
    r = num_correct_pred / num_truth
    return (p, r, (2 * p * r) / (p + r))

class MetricLayerF1(tf.keras.layers.Layer):

    def __init__(self):
        super(MetricLayerF1, self).__init__()
        self.metric_mean_fns = []

    def build(self, input_shape):
        self.m_p = tf.keras.metrics.Mean("precision")
        self.m_r = tf.keras.metrics.Mean("recall")
        self.m_f1 = tf.keras.metrics.Mean("F1")
        super(MetricLayerF1, self).build(input_shape)

    def get_config(self):
        return {"vocab_size": 5}

    def call(self, inputs):
        preds, targets = inputs[0], inputs[1]
        preds = tf.sigmoid(preds)
        p,r, f1 = f1score(targets, preds)
        self.add_metric(self.m_p(p))
        self.add_metric(self.m_r(r))
        self.add_metric(self.m_f1(f1))
        return preds
    
class MetricLayer(tf.keras.layers.Layer):

    def __init__(self):
        super(MetricLayer, self).__init__()
        self.metric_mean_fns = []

    def build(self, input_shape):
        self.metric_mean_fns = [
            (tf.keras.metrics.Mean("Acc"), accuracy),
            (tf.keras.metrics.Mean("RecallWake"), accuracy_wake),
            (tf.keras.metrics.Mean("RecallN1"), accuracy_n1),
            (tf.keras.metrics.Mean("RecallN2"), accuracy_n2),
            (tf.keras.metrics.Mean("RecallN3"), accuracy_n3),
            (tf.keras.metrics.Mean("RecallREM"), accuracy_rem),
        ]
        #self.metric_fn = tf.keras.metrics.SparseCategoricalAccuracy()
        super(MetricLayer, self).build(input_shape)

    def get_config(self):
        return {"vocab_size": 5}

    def call(self, inputs):
        logits, targets = inputs[0], inputs[1]
        for mean, fn in self.metric_mean_fns:
            m = mean(fn(logits, targets))
            self.add_metric(m)

        return logits
