from __future__ import print_function
import argparse
import torch
import torch.utils.data
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import xavier_uniform_
from torch.nn.init import xavier_normal_
from torch.nn.init import kaiming_uniform_
from torch.nn.init import kaiming_normal_
from torch.nn.modules import BatchNorm1d
import torch.optim as optim
import torch.autograd as autograd
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Function
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tensorboardX import SummaryWriter
from collections import namedtuple
from scipy.io import loadmat
import numpy as np
import time
import pdb

writer = SummaryWriter()

def load_data(mat_file, data_dir="data/"):
    data = loadmat(data_dir+mat_file);
    X, Y = data["X"], data["Y"];
    X = X.flatten();
    Y = Y.flatten();
    return (X, Y);

class BCGDataset(Dataset):
    """
    """
    def __init__(self, mat_file, is_train=True, seqlen=500):
        """
        set seqlen to 10s
        """
        self.seqlen = seqlen;
        X, Y = load_data(mat_file);
        X = torch.from_numpy(X);
#        Y = torch.from_numpy(Y);
        if is_train:
            X = X[:100000]
            Y = Y[:100000]
        else:
            X = X[100000:]
            Y = Y[100000:]
        self.X = X.type(torch.FloatTensor);
        self.Y = Y;

    def __len__(self):
        return self.X.shape[0]-self.seqlen+1;

    def find_peak_position(self, y):
#        assert sum(y) <= 1
        inds = []
        for (ind, val) in enumerate(y):
            if (val == 1):
                inds.append(ind+1)
        if len(inds) == 0:
            return 0;
        elif len(inds) == 1:
            return inds[0];
        else:
            ret = 0
            for ind in inds:
                if np.abs(ind-250) < ret:
                    ret = ind;
            return ret;
    
    def __getitem__(self, ind):
        x = self.X[ind:ind+self.seqlen];
        y = self.Y[ind:ind+self.seqlen];
        x = x.view(1, -1);
        ind = self.find_peak_position(y)
        label = torch.tensor(ind).type(torch.float32)
        return (x, label);


class VGGNet(nn.Module):
    def __init__(self):
        super(VGGNet, self).__init__();
        #1x500
        self.conv1 = nn.Conv1d(1, 8, kernel_size=3, stride=2);
        #8x249
        self.bn1   = BatchNorm1d(8);
        self.conv2 = nn.Conv1d(8, 8, kernel_size=4, stride=2);
        #8x123
        self.bn2   = BatchNorm1d(8);
        self.conv3 = nn.Conv1d(8, 8, kernel_size=4, stride=4);
        self.bn3   = BatchNorm1d(8);
        #8x30
        self.conv4 = nn.Conv1d(8, 4, kernel_size=3, stride=2);
        self.bn4   = BatchNorm1d(4);
        #16x14
        self.conv5 = nn.Conv1d(4, 4, kernel_size=3, stride=2);
        self.bn5   = BatchNorm1d(4);
        #16x6
        self.conv6 = nn.Conv1d(4, 4, kernel_size=6, stride=1);
        self.bn6   = BatchNorm1d(4);
        #16x1
        self.conv7 = nn.Conv1d(4, 1, kernel_size=1, stride=1);
        #1x1
                
    def forward(self, x):
        Debug = False;
        if Debug:
            print(x.shape)
        x = self.define_layer(x, self.conv1, self.bn1, debug=Debug);
        x = self.define_layer(x, self.conv2, self.bn2, debug=Debug)
        x = self.define_layer(x, self.conv3, self.bn3, debug=Debug)
        x = self.define_layer(x, self.conv4, self.bn4, debug=Debug)
        x = self.define_layer(x, self.conv5, self.bn5, debug=Debug)
        x = self.define_layer(x, self.conv6, self.bn6, debug=Debug)
        x = self.define_layer(x, self.conv7, nonlinearity='hardtanh', debug=Debug)
        x = torch.squeeze(x);
        return x;
    
    def define_layer(self, x, conv, bn=None, nonlinearity="relu", debug=False):
        z = conv(x)
        bn = None;
#        if bn is not None:
#            z = bn(z);
        if nonlinearity == 'relu':
            x = F.relu(z);
        elif nonlinearity == 'leaky_relu':
            x = F.leaky_relu(z);
        elif nonlinearity == 'hardtanh':
            hardtanh = nn.Hardtanh(0,500)
            x = hardtanh(z);
        else:
            raise NotImplementedError("{} not implemented".format(nonlinearity))
        if debug:
            print(x.shape)
        return x;

def xavier_uniform_weights_init(m):
    if isinstance(m, nn.Conv1d):
        xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain("relu"));

def xavier_uniform_weights_init(m):
    if isinstance(m, nn.Conv1d):
        xavier_normal_(m.weight.data, gain=nn.init.calculate_gain("relu"));

def kaiming_normal_weights_init(m):
    if isinstance(m, nn.Conv1d):
        kaiming_normal_(m.weight.data, nonlinearity="relu");

def kaiming_uniform_weights_init(m):
    if isinstance(m, nn.Conv1d):
        kaiming_uniform_(m.weight.data, nonlinearity="relu");
        
def cnn_eval_accuracy(preds, truths):
    return (preds - truths).abs().mean();

def confusion_matrix(preds, truths):
    preds = preds > 0.5;
    peak_peak       = (preds[truths==1] == 1).sum();
    peak_nonpeak    = (preds[truths==1] == 0).sum();
    nonpeak_peak    = (preds[truths==0] == 1).sum();
    nonpeak_nonpeak = (preds[truths==0] == 0).sum();
    return peak_peak, peak_nonpeak, nonpeak_peak, nonpeak_nonpeak;

def print_confusion_matrix(preds, truths):
    peak_peak, peak_nonpeak, nonpeak_peak, \
        nonpeak_nonpeak = confusion_matrix(preds, truths);
    print("T\P        Peak\t  NonPeak\n")
    print("Peak:{:>10}\t{:>10}\n".format(peak_peak,
                                       peak_nonpeak))
    print("NonPeak:{:>7}\t{:>10}\n".format(nonpeak_peak,
                                          nonpeak_nonpeak))
#    print("Sum:\t{}\t{}".format(peak_peak+nonpeak_peak))
        
def train(model, device, train_loader, val_loader, optimizer, epoch):
    model.train();
    criterion = nn.MSELoss();
    dataset_len = len(train_loader)
    loss = 0
    for batch_ind, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device);
        optimizer.zero_grad();
        output = model(data);
        loss   = criterion(output, target);
        loss.backward();
        optimizer.step();
        if batch_ind % 30 == 0:
            offset = cnn_eval_accuracy(output, target);
            val_loss, val_offset = validation(model, device,
                                              val_loader, criterion);
            writer.add_scalars("MSELoss",
                               {"train": loss, "validation": val_loss},
                               epoch * dataset_len + batch_ind)
            writer.add_scalars("offset_from_truth",
                              {"train":offset, "validation": val_offset},
                              epoch * dataset_len + batch_ind)
            Debug = False;
            if Debug:
                print("train_loss: {}, val_loss: {}, train_offset: {}, val_offest: {}".format(
                    loss, val_loss, offset, val_offset))
                #print_confusion_matrix(output, target)
    return loss;
        
def validation(model, device, val_loader, criterion):
    model.eval();
    val_loss   = 0;
    val_offset = 0;
    i = 0;
    with torch.no_grad():
        for data, target in val_loader:
            i += 1;
            data, target = data.to(device), target.to(device);
            output = model(data);
            val_loss += criterion(output, target);
            val_offset += cnn_eval_accuracy(output, target);
#            if i == 1:
#                break;
    val_loss   /= i;
    val_offset /= i;
#    model.train();
    return val_loss, val_offset;

def get_optimizer(name, model, hp):
    """    
    """
    if name == "Adam":
        optimizer = optim.Adam(model.parameters(),
                               lr=hp.lr,
                               weight_decay=hp.weight_decay)
        return optimizer;
    if name == "Nesterov":
        optimizer = optim.SGD(model.parameters(),
                              lr=hp.lr,
                              momentum=hp.momentum,
                              weight_decay=hp.weight_decay,
                              nesterov=True);
        return optimizer;
    if name == "SGD":
        optimizer = optim.SGD(model.parameters(),
                              lr=hp.lr,
                              momentum=hp.momentum,
                              weight_decay=hp.weight_decay,
                              nesterov=False);
        return optimizer;
    raise NotImplementedError("optimizer {} not implemented".format(name))
    
    

def main():
    torch.manual_seed(2018);
    HP = namedtuple("HyperParameters", ["use_cuda", "epochs", "lr",
                                        "momentum", "weight_decay", 'batch_size']);
    hp = HP(use_cuda=True, epochs=15000, lr=0.001, momentum=0.9, weight_decay=1e-6,
            batch_size=8192*6+2048*2+1024*4);
    device   = torch.device("cuda" if hp.use_cuda else "cpu");
    mat_file = "glt_labeled.mat";
    train_dataset = BCGDataset(mat_file, is_train=True);
    train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=hp.batch_size,
                                                shuffle=True, num_workers=6);
    val_dataset = BCGDataset(mat_file, is_train=False);
    val_loader  = torch.utils.data.DataLoader(val_dataset, batch_size=hp.batch_size,
                                              shuffle=True, num_workers=6);
    model     = VGGNet().to(device);
    model.apply(kaiming_normal_weights_init);
    optimizer = get_optimizer("Adam", model, hp);
    scheduler = ReduceLROnPlateau(optimizer, "min", verbose=True);
    for epoch in range(hp.epochs):
        loss = train(model, device, train_loader, val_loader, optimizer, epoch);
        scheduler.step(loss);
if __name__ == '__main__':
    main();
