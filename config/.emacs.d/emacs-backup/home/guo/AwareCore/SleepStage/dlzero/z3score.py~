from __future__ import print_function
import argparse
import torch
import torch.utils.data
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn.init import xavier_uniform_
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Function
from collections import namedtuple
from collections import Counter
from tensorboardX import SummaryWriter
from scipy.signal import lfilter
from scipy.signal import stft
from scipy.signal import decimate
from scipy.io import loadmat
import numpy as np
import pandas as pd
import time
import pdb

from absl import flags
from absl import logging



writer = SummaryWriter("logs")
    
class SleepDataset(Dataset):
    """
    """
    def __init__(self, data_dir, is_train=True):
        self.segment_len = 30; #30s
        self.resolution = 100
        sample_file = "n1.edf";
        # data: (3, signal_len) 100Hz
        data = load_edf(data_dir + sample_file);
        label_file = "n1.txt";
        labels = load_label(data_dir + label_file);
        if is_train:
            self.labels = labels[:25000]
            self.data = data[:,:25000*100]
        else:
            self.labels = labels[25000:30000]
            self.data = data[:,25000*100:30000*100]

    def __len__(self):
        return int(self.data.shape[1]/100-30+1);

    def __getitem__(self, ind):
        #NCHW
        # (batch_size, 3, 32freqbins, 32timepoints);
        start_ind = ind*self.resolution;
        end_ind = (ind+self.segment_len)*self.resolution;
        signals = self.data[:, start_ind:end_ind];
        labels = self.labels[ind:ind+self.segment_len];
        label = labels[0];
        if np.unique(labels).size != 1:
            cnt = Counter();
            for num in labels:
                cnt[num] += 1;
            label = cnt.most_common()[0][0]
        x = torch.from_numpy(np.stack([c4a1, e1a2, e2a1])).type(torch.float32);
        y = torch.tensor(label);
        return x, y;

class Z3ScoreCNNNet(nn.Module):
    """
    z3score Net based on 
    """
    def __init__(self):
        super(Z3ScoreCNNNet, self).__init__();
        self.conv1 = nn.Conv2d(3,  32, kernel_size=(3,3));
        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3));
        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2);
        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3,3));
        self.conv5 = nn.Conv2d(64, 64, kernel_size=(3,3));
        self.conv6 = nn.Conv2d(64, 64, kernel_size=(3,3), stride=2);
        self.conv7 = nn.Conv2d(64, 64, kernel_size=(4,4));
        self.conv8 = nn.Conv2d(64, 5,  kernel_size=(1));
        self.log_softmax = nn.LogSoftmax(dim=1)
        
    def forward(self, x):
        x = F.relu(self.conv1(x));
        x = F.relu(self.conv2(x));
        x = F.relu(self.conv3(x));
        x = F.relu(self.conv4(x));
        x = F.relu(self.conv5(x));
        x = F.relu(self.conv6(x));
        x = F.relu(self.conv7(x));
        x = self.conv8(x)
        x = torch.squeeze(x)
        x = self.log_softmax(x);
        return x;
    
def xavier_weights_init(m):
    if isinstance(m, nn.Conv2d):
        xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain("relu"))

def write_summaries(output, target, loss, val_loss, niter):
    nllloss = {"train": loss,"val": val_loss};
    writer.add_scalars("NLLLoss", nllloss, niter);
    class_summaries(output, target);

def train(model, device, train_loader, val_loader, optimizer, criterion, epoch):
    model.train();
    dataset_len = len(train_loader);
    for batch_ind, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device);
        optimizer.zero_grad();
        output = model(data);
        loss = criterion(output, target);
        loss.backward();
        optimizer.step();
        if batch_ind % 3 == 0:
            val_loss = validation(model, device, val_loader, criterion);
            model.train();
            niter = epoch * dataset_len + batch_ind;
            write_summaries(output, target, loss, val_loss, niter);
    
def validation(model, device, val_loader, criterion):
    model.eval();
    val_loss = 0;
    i = 0
    with torch.no_grad():
        for data, target in val_loader:
            i += 1;
            data, target = data.to(device), target.to(device);
            output = model(data);
            val_loss += criterion(output, target);
    val_loss /= i;
    return val_loss


def run_dlzero(data_dir, hp):
    torch.manual_seed(2018);
    device = torch.device("cuda" if hp.use_cuda else "cpu");
    train_dataset = CAPSleepDataset(data_dir, is_train=True);
    val_dataset   = CAPSleepDataset(data_dir, is_train=False);
    train_loader  = torch.utils.data.DataLoader(train_dataset,
                                                batch_size=hp.batch_size_cnn, num_workers=12);
    val_loader  = torch.utils.data.DataLoader(val_dataset,
                                              batch_size=hp.batch_size_cnn, num_workers=12);
    cnn = Z3ScoreCNNNet().to(device);
    cnn.apply(xavier_weights_init);
    optimizer = optim.SGD(cnn.parameters(),
                          lr=hp.lr,
                          momentum=hp.momentum,
                          weight_decay=hp.weight_decay,
                          nesterov=True);
    criterion = nn.NLLLoss();
    for epoch in range(hp.epochs):
        train(cnn, device, train_loader, val_loader, optimizer, criterion, epoch);


def main(args):
    del args
    data_dir = "../data/capslpdb/";
    HP = namedtuple("HyperParameters", ["use_cuda", "epochs", "lr", "momentum", "weight_decay",
                                        "batch_size_cnn", "batch_size_mlp"]);
    hp = HP(use_cuda=True, epochs=1500, lr=0.001, momentum=0.9, weight_decay=1e-6,
            batch_size_cnn=2**10*3, batch_size_mlp=1000);
    run_divesleepstage(data_dir, hp)

if __name__ == "__main__":
    app.run(main)

