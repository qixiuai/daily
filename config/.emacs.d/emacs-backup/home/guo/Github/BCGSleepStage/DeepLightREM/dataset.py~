
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

import os
import pdb
import glob
import time
import random
import pickle
import peakutils
import shutil
import autopeaks
import numpy as np
import matplotlib.pyplot as plt

from collections import deque
from scipy.io import loadmat
from scipy.signal import medfilt
from scipy.signal import detrend
from scipy.interpolate import interp1d


class RawDataset(object):
    """ raw dataset """
    
    def __init__(self):
        self.database_dir = "/home/guo/data/sleepstage/database/"

def _mat2labelpath(matpath):
    subject_id = os.path.basename(matpath)[7:-4]
    dirname = os.path.dirname(matpath)
    dirname = dirname.replace("Extractedchannels", "")
    dirname = dirname.replace("ExtractedChannels", "")
    dirname = dirname.replace("-", "")
    # ...subgroup1/1/1_1.txt
    labelpath = dirname + "/" + str(subject_id) + "/" + str(subject_id) + "_1.txt"
    return labelpath


class IsrucDataset(RawDataset):

    def __init__(self, train=True):
        super().__init__()
        self._isruc_dir = self.database_dir + "ISRUC_Sleep/ExtractedChannels/"
        self.autopeaks = autopeaks.AutoPeaks(thres=0.75, min_dist=110, fs=200)
        self.tmp_dir = "/home/guo/DukeCNN/data/"
        self.is_train = train
        
    def load_labels(self, matpath):
        labelpath = _mat2labelpath(matpath)
        label = np.loadtxt(labelpath)
        label = np.int64(label != 0)
        return label

    def extract_heart_rates_interpolated(self, ecg):
        """ interpolate the 5mins hrs to 4Hz signal """
        hrs_inp = []
        try:
            hrs = self.extract_heart_rates(ecg)
            hrs = medfilt(hrs, kernel_size=5)
            cycles = [60 / hr for hr in hrs]
            time_axis = np.cumsum(cycles)
            f = interp1d(time_axis, hrs, kind='cubic', fill_value="extrapolate")
            offset = 0
            newtime_axis = np.linspace(offset, 30+offset, num=30*4) # 120s
            #pdb.set_trace()
            hrs_inp = f(newtime_axis)
        except:
            pdb.set_trace()
        return hrs_inp

    def produce_heart_rates_example(self, file_path):
        epochs = loadmat(file_path)['X2'] # ecg
        labels = self.load_labels(file_path)
        num_epochs = epochs.shape[0]
        hrs_deque = deque(maxlen=1)
        #ecg_deque = deque(maxlen=10)
        label_deque = deque(maxlen=1)
        for ind in range(num_epochs):
            ecg = epochs[ind, :]
            label = labels[ind]
            # only generate sleep epochs
            if label == 0: 
                continue
            if label == 1:
                label = 0
            elif label == 2:
                label = 0
            elif label == 3:
                label = 1
            elif label == 4:
                label = 2
            hrs_epoch = self.extract_heart_rates_interpolated(ecg)
            if len(hrs_epoch) < 120:
                print("X", end=",")
                continue
            hrs_deque.append(hrs_epoch)
            label_deque.append(label)
            # make sure deque is full
            #if ind < 10:
            #    continue
            hrs = np.concatenate(hrs_deque)
            #hrs = self.extract_heart_rates_interpolated(ecg_raw)
            #if len(hrs) < 50:
            #    print("X",end="", flush=True)
            #    continue
            hrs = hrs.reshape(-1, 1)
            label = label_deque[-1]
            label = tf.one_hot(label, 3)
            yield (hrs,label)

    def extract_heart_rates(self, ecg):
        for val in ecg:
            self.autopeaks.findpeaks(val)
        ipks = self.autopeaks.peak_indexes
        hrs = 60 / (np.diff(ipks) / 200)
        return hrs

    def generate_dataset(self, debug=True):
        raw_mat_files = glob.glob(self._isruc_dir + "*/**.mat")#[:10]
        out_dir = self.tmp_dir
        if os.path.isdir(out_dir):
            print("remove directory: {}".format(out_dir))
            shutil.rmtree(out_dir)
        os.mkdir(out_dir)
        i = 0
        for file in raw_mat_files:
            examples = self.produce_heart_rates_example(file)
            subject_id = file[64:-4]
            subject_id = subject_id.replace("/", "")
            for example in examples:
                example = {"X": example[0], "label": example[1]}
                label = example["label"]
                with open(out_dir+"/"+str(i) + "_" + subject_id + "_"+str(label), "wb") as f:
                    pickle.dump(example, f)
                i += 1
                if i % 1000 == 0:
                    print("genrated {} examples".format(i))

    def __call__(self, ecg=True):
        light_sleep_examples = glob.glob(self.tmp_dir+"/**_0")
        deep_sleep_examples = glob.glob(self.tmp_dir+"/**_1")
        rem_sleep_examples = glob.glob(self.tmp_dir+"/**_2")
        num_min_stage_examples = min(len(light_sleep_examples),
                                     len(deep_sleep_examples),
                                     len(rem_sleep_examples))
        random.seed(2019)
        random.shuffle(light_sleep_examples)
        random.shuffle(deep_sleep_examples)
        random.shuffle(rem_sleep_examples)
        num_selected = int(num_min_stage_examples * 0.8)
        if self.is_train:
            light_sleep_examples = light_sleep_examples[:num_min_stage_examples]
            deep_sleep_examples = deep_sleep_examples[:num_min_stage_examples]
            rem_sleep_examples = rem_sleep_examples[:num_min_stage_examples]
        else:
            light_sleep_examples = light_sleep_examples[num_min_stage_examples:]
            deep_sleep_examples = deep_sleep_examples[num_min_stage_examples:]
            rem_sleep_examples = rem_sleep_examples[num_min_stage_examples:]
        files = light_sleep_examples + deep_sleep_examples + rem_sleep_examples
        random.shuffle(files)
        #print("num examples used: {}".format(len(files)))
        for file in files:
            with open(file, "rb") as f:
                example = pickle.load(f)
                X = example["X"]
                # normalize each example by substract its median
                X = X - np.median(X)
                yield (X, example['label'])


if __name__ == '__main__':
    isruc = IsrucDataset()
    generate_data = True
    if generate_data:
        isruc.generate_dataset()
    dataset = tf.data.Dataset.from_generator(isruc, (tf.float32, tf.int64))
    dataset = dataset.shuffle(buffer_size=64).batch(64)
    mean = 0
    count = 0
    for x, y in dataset:
        ratio = np.sum(y) / 64.0
        #pdb.set_trace()
        mean = (mean * count + ratio) / (count + 1)
        print( "sleep epoch ratio: {}, mean ratio: {}".format(ratio, mean))
        count += 1

